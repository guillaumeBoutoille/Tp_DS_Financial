{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP: Datascience pour la finance\n",
    "\n",
    "### Problème de Merton\n",
    "\n",
    "On a un actif $S_A$ qui se modélise par l'équation suivante :\n",
    "\n",
    "$$ dS_A = \\sigma \\; S_A \\; dW_A $$\n",
    "\n",
    "Pour cet exercice on utilise la fonction d'utilité suivante :\n",
    "$$\n",
    "U(X) = 1-e^{-X}\n",
    "$$\n",
    "\n",
    "On fait l'hypothèse que la stratégie optimale est une fonction polynomiale de degré au plus D.\n",
    "La stratégie $\\alpha$ consiste à :\n",
    "- Initialisation : acheter $\\alpha (0)$ en $T=0 $\n",
    "- En $T=n$ avec $n\\geq 1$ : On vend $\\alpha (n-1) $ et on achète $\\alpha(n) $ tel que \n",
    "$$\n",
    "\\alpha(n) = \\sum_{0 \\leq k\\leq D} \\alpha^{(k)}(n) \\; (S_{T_n})^k\n",
    "$$\n",
    "\n",
    "La fonction $V$ représente notre richesse en fonction du temps $T_N$, par rapport a notre stratégie $\\alpha$:\n",
    "$$\n",
    "V^{\\alpha,D}(T_N) = \\sum_{i=0}^{N-1}  \\alpha(i) \\; (S_{T_{i+1}} - S_{T_i}) + \\mathbb{1}_{S_{T_N} \\geq K}\n",
    "$$\n",
    "\n",
    "On cherche la stratégie qui maximise notre fonction d'utilité, i.e, on résoud le problème suivant :\n",
    "\n",
    "$$\n",
    " \\max_{\\alpha} \\mathbb{E} [ U(V^{\\alpha,D}(T_n)) ]\n",
    "$$\n",
    "\n",
    "\n",
    "### Simulation du processus \n",
    "\n",
    "On prend comme hypothèse que :\n",
    "- $S_0 = 100$\n",
    "- $N = 100$\n",
    "- $\\sigma = 10 \\%$\n",
    "- $K = [80-120] $\n",
    "- $r=0$\n",
    "\n",
    "On simule le processus du sous-jacent par la formule suivante:\n",
    "\n",
    "$$ S_T = S_0 \\cdot \\exp  ( -\\frac{\\sigma^2}{2}\\cdot T + \\sigma W_T )  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S0 = 10\n",
    "N = 10\n",
    "sigma = 0.1\n",
    "T = np.linspace(0,1,100)\n",
    "\n",
    "def Brownian_Motion(n,N,sigma):\n",
    "    \"\"\"\n",
    "    - n représente le nombre de données que l'on veut générer pour l'échantillon\n",
    "    - N est la longueur de notre mouvement brownien.\n",
    "    - sigma représente l'écart-type.\n",
    "    On retroune W la matrice des n mouvements browniens.\n",
    "    \"\"\"\n",
    "    G = np.random.normal(loc = 0,scale = sigma,size = (n,N))\n",
    "    zeros = np.zeros(n).reshape(n,1)\n",
    "    W = np.concatenate((zeros,np.cumsum(G,axis = 1)),axis=1)\n",
    "    return W\n",
    "\n",
    "def S_t(S0=S0, N=N,sigma=sigma,n=10):\n",
    "    \"\"\"\n",
    "    - S0 représente la première valeur de notre courbe de taux que l'on simule\n",
    "    On retourne n simulations de la courbe de taux décrite par l'EDP (1).\n",
    "    \"\"\"\n",
    "    return np.multiply(np.repeat(S0,n).reshape(n,1),np.exp(-0.5*sigma**2*np.arange(N+1)/(N+1)+sigma*Brownian_Motion(n,N,sigma))).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On génère un échantillon de données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25e4ff50e48>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMMYYMyQLFMYYY4b0fwCE7aIboxXkKwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25e4fea8a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "S = S_t(S0=S0,N=N,sigma = sigma ,n=1)\n",
    "plt.plot(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Algorithme stochastique\n",
    "\n",
    "On applique une descente de gradient pour trouver le $\\alpha^{\\star}$. Le principe est d'initialiser les paramètres à des valeurs quelconques puis de se rapprocher de la solution en itérant dans le sens inverse du gradient:\n",
    "$$\n",
    "\\alpha_{i+1}(T_n) = \\alpha_i(T_n) -  \\theta_n\\cdot\\nabla_{\\alpha_i} U(V^{\\alpha,D}(T_n))\n",
    "$$\n",
    "\n",
    "avec $\\nabla_{\\alpha_i} U(V^{\\alpha,D}(T_n)) = \\frac{\\partial U(V^{\\alpha,D})}{\\partial \\alpha_i}(\\cdot) $, la dérivée partiel par rapport au paramètre à optimiser et $\\theta_n$ est le pas de descente.\n",
    "\n",
    "Dans notre cas on va choisir $\\theta_n = 1/n$.\n",
    "\n",
    "###### Petit rappel de calcul des dérivées :\n",
    "\n",
    "$$\n",
    "    (f(g(x)))^{\\prime} = g^{\\prime}(x)\\times f^{\\prime}(g(x))\n",
    "$$\n",
    "\n",
    "Un petit rappel de l'énoncé:\n",
    "$$\n",
    "    V^{\\alpha,D}(T_N) = \\sum_{i=0}^{N-1}  \\alpha(i) \\; (S_{T_{i+1}} - S_{T_i}) + \\mathbb{1}_{S_{T_N} \\geq K}\n",
    "$$\n",
    "Donc, on a\n",
    "$$\n",
    "    \\nabla_{\\alpha_i} U(V^{\\alpha,D}(T_i)) = \n",
    "    \\left \\{\n",
    "   \\begin{array}{r l}\n",
    "      \\exp(-V^{\\alpha,D}(0))\\cdot (S(T_1)-S(0)) & \\text{ si } T_i = 0 \\\\\n",
    "      \\exp(-V^{\\alpha,D}(T_i))\\cdot (S(T_{i+1})-S(T_i)) & \\text{ si } T_i > 0 \\text{ et } k=0 \\\\\n",
    "      \\exp(-V^{\\alpha,D}(T_i)) \\cdot (S_{T_i})^k\\cdot (S(T_{i+1})-S(T_i)) & \\text{ sinon } \n",
    "   \\end{array}\n",
    "   \\right .\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parameters_alpha(alpha,S,D):\n",
    "    return np.r_[alpha[0],np.diag(alpha[1::].reshape(len(S)-1,-1).dot(np.array([S[1:(len(S)-1)]**i for i in range(D+1)]).reshape(D+1,-1)))]\n",
    "\n",
    "def V(alpha,S,D,k):\n",
    "    return np.r_[alpha[0]*S[0], np.repeat(np.cumsum(parameters_alpha(alpha,S,D)[0:(len(S)-1)]*(S[1::]-S[0:(len(S)-1)]).ravel()).ravel(),D+1)]+np.repeat((S>k).ravel(),D+1)[D::]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(alpha,S,learning_rate,D,k):\n",
    "    \"\"\"\n",
    "    - alpha sont les paramètres a optimiser\n",
    "    - S le processus stochastique\n",
    "    - learning_rate le pas de descente\n",
    "    - D le degré des polynomes alpha\n",
    "    On retourne le gradient et les nouveaux parmaètres après une itération.\n",
    "    \"\"\"\n",
    "    grads = np.exp(-V(alpha ,S[0:(len(S)-1)] ,D ,k))*np.r_[S[1]-S[0],np.multiply(np.array([S[1:(len(S)-1)]**i for i in range(D+1)]),S[2:]-S[1:(len(S)-1)]).T.ravel()]\n",
    "    alpha -= learning_rate * grads\n",
    "    return (grads,alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimization(alpha, S, learning_rate,D,k, num_iteration):\n",
    "    \"\"\"\n",
    "    - alpha sont les paramètres a optimiser\n",
    "    - S le processus stochastique\n",
    "    - learning_rate le pas de descente\n",
    "    - D le degré des polynomes alpha\n",
    "    On applique la descente de gradient pour un certain nombre d'itération et on retourne les paramètres optimisés.\n",
    "    \"\"\"\n",
    "    for i in range(num_iteration):\n",
    "        grads,alpha = gradient_descent(alpha,S,learning_rate/num_iteration,D,k)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.          1.12944382  1.19752105  1.03657689  1.03117481  1.08910914\n",
      "  1.01216784  1.03422458  1.04055757  1.03642542]\n",
      "[ 2.16193196e-06  2.20034449e-02 -4.85957691e-02 -1.91593689e-03\n",
      "  2.06586745e-02 -2.58919876e-02  8.01608687e-03  2.25138997e-03\n",
      " -1.45971565e-03 -7.82561046e-03] [0.99999784 0.97799656 1.04859577 1.00191594 0.97934133 1.02589199\n",
      " 0.99198391 0.99774861 1.00145972 1.00782561]\n"
     ]
    }
   ],
   "source": [
    "D=0\n",
    "alpha = np.ones((D+1)*(N-1)+1).ravel()\n",
    "print(V(alpha,S[0:(len(S)-1)],D,6))\n",
    "\n",
    "grads, alpha = gradient_descent(alpha,S,learning_rate=1,D=D,k=6)\n",
    "\n",
    "print(grads,alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99999196 0.91818495 1.18115788 1.00723939 0.92193952 1.09803984\n",
      " 0.96954099 0.99144265 1.0055484  1.02974556]\n"
     ]
    }
   ],
   "source": [
    "print(optimization(alpha,S,learning_rate=1,D=D,k=S0+1,num_iteration=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reseau de neurones \n",
    "\n",
    "Le but du réseau de neurone est de maximiser une fonction d'energie. Dans notre cas c'est la fonction d'utilité. On lui donne la courbe de l'actif sur lequel on cherche la stratégie optimale et le reseau de neurone cherche les paramètres optimaux $\\alpha^{\\star}$.\n",
    "\n",
    "<img src=\"NN.png\">\n",
    "\n",
    "Pour construire un réseau de neurones on réalise plusieurs fonctions. \n",
    "La première est celle de l'initialisation des paramètres que l'on cherche à optimiser avec le réseau de neurone.\n",
    "On rappel que les paramètres à optimiser s'exprime de la manière suivante :\n",
    "\n",
    "$$ \n",
    "    \\alpha(i) = \\left \\{\n",
    "   \\begin{array}{r l}\n",
    "      \\alpha(0) & \\text{ si } i = 0 \\\\\n",
    "      \\sum_{0 \\leq k\\leq D} \\alpha^{(k)}(i) \\; (S_{T_i})^k & \\text{ si } i > 0 \n",
    "   \\end{array}\n",
    "   \\right .\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Initialisation du réseau de neurone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Initialisation(N,D):\n",
    "    \"\"\"\n",
    "    On initialise les paramètres.\n",
    "    - N : Nombre de Dates où l'on optimise notre quantité d'actif\n",
    "    - D : Le degrée du polynôme de la quantité d'actif\n",
    "    \"\"\"\n",
    "    a = np.ones((D+1)*(N-1)+1)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension de a = (10,)\n"
     ]
    }
   ],
   "source": [
    "a  = Initialisation(N,D)\n",
    "\n",
    "print(\"dimension de a = \"+str(a.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La propagation et rétropropagation \n",
    "\n",
    "Cette étape permet d'atapter la descente de gradient à la structure du neurone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_propagation(alpha,S,N,D,k):\n",
    "    \"\"\"\n",
    "    - alpha : paramètres à optimiser\n",
    "    - S : courbe de l'actif\n",
    "    - N : Date à laquelle on fait les échanges\n",
    "    - D : Degrée de polynome pour approcher la stratégie optimale\n",
    "    \"\"\"\n",
    "    # foward propagation \n",
    "    U = 1-np.exp(-V(alpha,S[0:(len(S)-1)],D,k))\n",
    "    \n",
    "    # backward propagation\n",
    "    grads = np.exp(-V(alpha ,S[0:(len(S)-1)] ,D ,k))*np.r_[S[1]-S[0],np.multiply(np.array([S[1:(len(S)-1)]**i for i in range(D+1)]),S[2:]-S[1:(len(S)-1)]).T.ravel()]\n",
    "    \n",
    "    return grads, U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_neural_network(S,N,D,k, num_iterations, learning_rate=1):\n",
    "    alpha = Initialisation(N,D)\n",
    "    utilite = []\n",
    "    for i in range(num_iterations):\n",
    "        grads, U = compute_propagation(alpha,S,N,D,k)\n",
    "        \n",
    "        # update parameters\n",
    "        alpha = alpha - (learning_rate/(i+1))*grads\n",
    "        \n",
    "        if i %100 == 0:\n",
    "            utilite.append(U[-1])\n",
    "            print(U[-1])\n",
    "    return alpha, grads, utilite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6452796045122907\n",
      "0.6208664583370982\n",
      "0.617452509797519\n",
      "0.6154339221339203\n",
      "0.6139924854736307\n",
      "0.6128692422247652\n",
      "0.6119481594349694\n",
      "0.6111670742640888\n",
      "0.6104887546115025\n",
      "0.609889117296054\n"
     ]
    }
   ],
   "source": [
    "alpha,grads,utilite = compute_neural_network(S,N,D,6, 1000, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Fonction Utilité : $U(X)=X^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(alpha,S,learning_rate,D,k):\n",
    "    \"\"\"\n",
    "    - alpha sont les paramètres a optimiser\n",
    "    - S le processus stochastique\n",
    "    - learning_rate le pas de descente\n",
    "    - D le degré des polynomes alpha\n",
    "    On retourne le gradient et les nouveaux parmaètres après une itération.\n",
    "    \"\"\"\n",
    "    grads = 2*V(alpha ,S[0:(len(S)-1)] ,D ,k)*np.r_[S[1]-S[0],np.multiply(np.array([S[1:(len(S)-1)]**i for i in range(D+1)]),S[2:]-S[1:(len(S)-1)]).T.ravel()]\n",
    "    alpha -= learning_rate * grads\n",
    "    return (grads,alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  11.            1.12944382    1.12944382    1.12944382    1.12944382\n",
      "   79.62746811   79.62746811   79.62746811   79.62746811 -109.58194336\n",
      " -109.58194336 -109.58194336 -109.58194336 -115.64730267 -115.64730267\n",
      " -115.64730267 -115.64730267  -50.70076035  -50.70076035  -50.70076035\n",
      "  -50.70076035 -138.40236176 -138.40236176 -138.40236176 -138.40236176\n",
      " -113.81107138 -113.81107138 -113.81107138 -113.81107138 -106.70531522\n",
      " -106.70531522 -106.70531522 -106.70531522 -111.35014763 -111.35014763\n",
      " -111.35014763 -111.35014763]\n",
      "[ 2.84776406e+00  1.53778822e-01  1.55769394e+00  1.57785732e+01\n",
      "  1.59828171e+02 -2.56311533e+01 -2.61374226e+02 -2.66536917e+03\n",
      " -2.71801582e+04  1.18394033e+00  1.18827081e+01  1.19261714e+02\n",
      "  1.19697936e+03 -1.33998978e+01 -1.34416717e+02 -1.34835759e+03\n",
      " -1.35256107e+04  7.80196410e+00  7.87148672e+01  7.94162886e+02\n",
      "  8.01239603e+03 -6.10540982e+00 -6.11283879e+01 -6.12027680e+02\n",
      " -6.12772386e+03 -1.44152860e+00 -1.44646217e+01 -1.45141263e+02\n",
      " -1.45638003e+03  8.81845951e-01  8.85422505e+00  8.89013563e+01\n",
      "  8.92619187e+02  4.91306895e+00  4.93096501e+01  4.94892625e+02\n",
      "  4.96695292e+03] [-1.84776406e+00  8.46221178e-01 -5.57693936e-01 -1.47785732e+01\n",
      " -1.58828171e+02  2.66311533e+01  2.62374226e+02  2.66636917e+03\n",
      "  2.71811582e+04 -1.83940328e-01 -1.08827081e+01 -1.18261714e+02\n",
      " -1.19597936e+03  1.43998978e+01  1.35416717e+02  1.34935759e+03\n",
      "  1.35266107e+04 -6.80196410e+00 -7.77148672e+01 -7.93162886e+02\n",
      " -8.01139603e+03  7.10540982e+00  6.21283879e+01  6.13027680e+02\n",
      "  6.12872386e+03  2.44152860e+00  1.54646217e+01  1.46141263e+02\n",
      "  1.45738003e+03  1.18154049e-01 -7.85422505e+00 -8.79013563e+01\n",
      " -8.91619187e+02 -3.91306895e+00 -4.83096501e+01 -4.93892625e+02\n",
      " -4.96595292e+03]\n"
     ]
    }
   ],
   "source": [
    "D=3\n",
    "alpha = np.ones((D+1)*(N-1)+1).ravel()\n",
    "print(V(alpha,S[0:(len(S)-1)],D,6))\n",
    "\n",
    "grads, alpha = gradient_descent_2(alpha,S,learning_rate=1,D=D,k=6)\n",
    "\n",
    "print(grads,alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.34122546e-01  8.57887176e-01 -4.39523871e-01 -1.35815762e+01\n",
      " -1.46703257e+02 -3.43111269e+03 -3.49980414e+04 -3.56902461e+05\n",
      " -3.63952956e+06  3.37049553e+06  3.38282285e+07  3.39519607e+08\n",
      "  3.40761463e+09  6.75382199e+08  6.77487689e+09  6.79599744e+10\n",
      "  6.81718383e+11  1.46983222e+12  1.48292977e+13  1.49614402e+14\n",
      "  1.50947603e+15  9.95683818e+14  9.96895350e+15  9.98108357e+16\n",
      "  9.99322840e+17 -4.33771119e+16 -4.35255683e+17 -4.36745327e+18\n",
      " -4.38240070e+19 -2.77038669e+17 -2.78162271e+18 -2.79290430e+19\n",
      " -2.80423164e+20  4.31967119e+18  4.33540577e+19  4.35119767e+20\n",
      "  4.36704709e+21]\n"
     ]
    }
   ],
   "source": [
    "print(optimization(alpha,S,learning_rate=1,D=D,k=S0+1,num_iteration=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_propagation(alpha,S,N,D,k):\n",
    "    \"\"\"\n",
    "    - alpha : paramètres à optimiser\n",
    "    - S : courbe de l'actif\n",
    "    - N : Date à laquelle on fait les échanges\n",
    "    - D : Degrée de polynome pour approcher la stratégie optimale\n",
    "    \"\"\"\n",
    "    # foward propagation \n",
    "    U = (V(alpha,S[0:(len(S)-1)],D,k))**2\n",
    "    \n",
    "    # backward propagation\n",
    "    grads = 2*V(alpha ,S[0:(len(S)-1)] ,D ,k)*np.r_[S[1]-S[0],np.multiply(np.array([S[1:(len(S)-1)]**i for i in range(D+1)]),S[2:]-S[1:(len(S)-1)]).T.ravel()]\n",
    "    \n",
    "    return grads, U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12398.855378035942\n",
      "6.484829051293037e+50\n",
      "9.614561185022915e+51\n",
      "3.752268719688901e+52\n",
      "9.147523338960497e+52\n",
      "1.7596334730772612e+53\n",
      "2.9387872761525255e+53\n",
      "4.471037089952552e+53\n",
      "6.368576332945101e+53\n",
      "8.639092941889934e+53\n"
     ]
    }
   ],
   "source": [
    "alpha,grads,utilite = compute_neural_network(S,N,D,6, 1000, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
